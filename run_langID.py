#!/usr/bin/env python3
"""
run_langID.py

Purpose:
This is the main script for performing line-by-line text extraction,
language identification (LID), and quality assessment.

It reads an input CSV (typically generated by 'alto_stats_create.py'),
which must contain 'file', 'page', and 'path' columns.

For each row (representing one page):
1.  It extracts all text lines from the ALTO XML file specified in the 'path'
    (using the `get_text_from_alto` utility).
2.  It runs the batch classification function (`classify_lines_batch`)
    on all lines from that page.
3.  This classification uses 'fastText' for language ID and 'distilgpt2'
    for perplexity (quality) scoring.
4.  It aggregates the results for the page (e.g., counting how many
    'Clear', 'Noisy', 'Trash' lines there are).

Outputs:
This script is configured to produce several outputs:

1.  OUTPUT_STATS_FILE (e.g., "line_counts_alto_statistics.csv"):
    The input CSV augmented with new columns for line counts
    (e.g., 'clear_lines', 'noisy_lines', 'trash_lines', 'languages').

2.  OUTPUT_LINES_FILE (e.g., "pages_classified_alto_statistics.csv"):
    A new, very large CSV containing one row for *every single text line*
    processed, detailing its text, language, perplexity, and category.

3.  OUTPUT_TEXT_DIR (e.g., "../PAGE_TXT"):
    A folder containing the raw extracted text (.txt) for each page,
    which acts as a cache.

4.  OUTPUT_STAT_DIR (e.g., "../PAGE_STAT"):
    A folder where the two main CSV outputs (1 and 2) are split into
    smaller, per-document files (e.g., "line_counts_doc123.csv").
"""

# Import all functions from our utility file
from text_util import *
import time  # For timing the process
import concurrent.futures

# --- Output Categories (Documentation) ---
# Each line of text is assigned one of the following categories:
# - Clear:    High-confidence, low-perplexity, common language.
# - Rough:    Medium-confidence, but still likely a common language.
# - Noisy:    Low-confidence, high-perplexity, or other indicators of OCR issues.
# - Trash:    Very high perplexity, or non-prose (e.g., all-caps headers).
# - Short:    Too few words (< 3) to make a confident classification.
# - Non-text: Failed heuristic checks (e.g., mostly digits/symbols).
# - Empty:    Line contains only whitespace.
# - N/A:      Used internally for error cases or placeholders.
# -------------------------------------------


# --- NEW HELPER FUNCTION TO PROCESS BATCH RESULTS ---
def process_batch_results(all_line_results: list[dict],
                          line_batch_metadata: list[dict],
                          lines_writer,
                          line_counts_cache: dict,
                          language_counts_cache: dict):
    """
    Takes the results from classify_lines_batch and writes them to the
    detailed CSV and updates the count caches.
    """

    for result, meta in zip(all_line_results, line_batch_metadata):
        file_id = meta['file_id']
        page_id = meta['page_id']
        page_key = (file_id, page_id)

        # 1. Write to the detailed lines CSV
        lines_writer.writerow([
            file_id, page_id, meta['line_number'], meta['original_line_text'],
            result.get('lang_code'), result.get('lang_corrected'),
            result.get('lang_score'), result.get('lang_score_corrected'),
            result.get('perplexity'), result.get('perplexity_corrected'),
            result.get('category'), result.get('corrected_text')
        ])

        # 2. Update the line_counts cache for this page
        category = result.get('category', 'Trash')  # Default to Trash on error
        if category in line_counts_cache.get(page_key, {}):
            line_counts_cache[page_key][category] += 1

        # 3. Update the language_counts cache for this page
        lang_code = result.get('lang_code', 'other')
        lang_saved = False
        page_lang_counts = language_counts_cache.get(page_key)

        if page_lang_counts is not None:
            for lang_name in page_lang_counts.keys():
                if lang_code.startswith(lang_name):
                    page_lang_counts[lang_name] += 1
                    lang_saved = True
                    break
            if not lang_saved:
                page_lang_counts['other'] += 1


def main():
    """
    (NEW, 16-LINE-BATCH VERSION)
    Main processing logic. Reads the input CSV sequentially, accumulates
    "worth-processing" lines into a fixed-size batch (e.g., 16),
    and processes them together.
    """
    # --- 1. Configuration ---
    INPUT_FILE = "test_alto_stats.csv"
    OUTPUT_TEXT_DIR = "../PAGE_TXT"
    OUTPUT_STAT_DIR = "../PAGE_STAT"
    CHUNK_SIZE = 500  # How many rows (pages) to read from CSV at a time
    LOG_STEP = 5  # How often to print detailed progress (now in *batches*)

    # --- NEW: BATCH_SIZE for lines ---
    # This is the number of *lines* to process in a single ML batch.
    BATCH_SIZE = 64

    MODEL_PATH = "lid.176.bin"
    OUTPUT_STATS_FILE = "line_counts_" + Path(INPUT_FILE).name
    OUTPUT_LINES_FILE = "pages_classified_" + Path(INPUT_FILE).name

    # --- 2. Load Spellers ---
    SPELLERS = {}
    for common_lang in COMMON_LANGS:
        speller_type, speller_lang = speller_per_language[common_lang]
        if speller_type == 1:
            SPELLERS[common_lang] = Speller(speller_lang, only_replacements=True)
        else:
            SPELLERS[common_lang] = SpellChecker(language=speller_lang)
    print(f"Spellcheckers initialized for languages: {', '.join(SPELLERS.keys())}")

    # --- 3. Setup ML Device ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device} | Processing {BATCH_SIZE} lines per batch.")

    # --- 4. Load Models ---
    print("Loading fastText model (fasttext)...")
    try:
        model_fasttext = fasttext.load_model(MODEL_PATH)
        print("fastText model loaded.")
    except ValueError as e:
        print(f"Error loading model: {e}", file=sys.stderr)
        print(f"Please ensure the model file '{MODEL_PATH}' is in the correct directory.", file=sys.stderr)
        sys.exit(1)

    print("Loading causal LM (distilgpt2) for perplexity...")
    model_id = "distilgpt2"
    model_causal = AutoModelForCausalLM.from_pretrained(model_id).to(device)
    tokenizer_causal = AutoTokenizer.from_pretrained(model_id)
    tokenizer_causal.pad_token = tokenizer_causal.eos_token
    print("Causal LM loaded.")

    # --- 5. Setup Output Directories ---
    print(f"Starting processing of '{INPUT_FILE}'")
    Path(OUTPUT_TEXT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"Raw text outputs will be saved to '{OUTPUT_TEXT_DIR}/'")
    Path(OUTPUT_STAT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"Document level results will be saved to '{OUTPUT_STAT_DIR}/'")

    print("Batch size for line processing:\t", BATCH_SIZE)

    try:
        # --- 6. Start Processing ---
        print(f"Analyzing '{INPUT_FILE}'...")
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1
        if total_rows <= 0:
            print("Input file is empty. Exiting.")
            return
        print(f"Found {total_rows:,} rows (pages) to process.")

        with open(OUTPUT_LINES_FILE, 'w', encoding='utf-8', newline='') as f_lines_csv:
            lines_writer = csv.writer(f_lines_csv)
            lines_writer.writerow(["file", "page", "line", "line_text",
                                   "lang_code", "lang_corrected",
                                   "lang_score", "lang_score_corrected",
                                   "perplexity", "perplexity_corrected",
                                   "category", "corrected_text"])

            rows_processed, txt_lines_processed = 0, 0
            batches_processed = 0

            all_stats_chunks = []  # Stores the *original* DataFrames
            page_stats_cache = {}  # Stores page-level stats for "Short" lines
            line_counts_cache = {}  # Stores line counts per page
            language_counts_cache = {}  # Stores lang counts per page

            line_batch_to_process = []  # Stores clean text for the next batch
            line_batch_metadata = []  # Stores context for the next batch

            print(f"Reading '{INPUT_FILE}' in chunks of {CHUNK_SIZE}...")
            reader = pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE)
            time_start = time.time()

            # --- 7. Chunk Loop ---
            for chunk_df in reader:
                if "path" not in chunk_df.columns:
                    raise ValueError("Input CSV must have 'path' column.")

                # Store the original chunk data for later
                all_stats_chunks.append(chunk_df)

                # --- 8. Row Loop (Page-by-Page) ---
                for idx, row in chunk_df.iterrows():
                    file_id = str(row['file'])
                    page_id = str(row['page'])
                    xml_path = row['path']
                    page_key = (file_id, page_id)  # Unique key for caches

                    # Define the path for the cached .txt file
                    xml_file_directory = Path(xml_path).parent
                    separate_dir = str(xml_file_directory).endswith(file_id)
                    if not separate_dir:
                        page_text_file = Path(OUTPUT_TEXT_DIR) / f"{file_id}-{page_id}.txt"
                    else:
                        page_text_file = Path(OUTPUT_TEXT_DIR) / file_id / f"{file_id}-{page_id}.txt"
                        page_text_file.parent.mkdir(parents=True, exist_ok=True)

                    # --- A. Extract Text from ALTO ---
                    page_lines = get_text_from_alto(xml_path, page_text_file)
                    total_text = " ".join(page_lines)

                    # --- B. Save raw text file (to complete the cache) ---
                    if not page_text_file.exists():
                        try:
                            with open(page_text_file, 'w', encoding='utf-8') as f_txt:
                                f_txt.write("\n".join(page_lines))
                        except Exception as e:
                            print(f"\n[Warning] Failed to write text file {page_text_file}: {e}", file=sys.stderr)

                    # --- C. Pre-calculate Page-Level stats and CACHE them ---
                    try:
                        total_text_perplexity = calculate_perplexity(total_text, model_causal, tokenizer_causal, device)
                        labels, scores = model_fasttext.predict(total_text, k=1)
                        total_lang_code = labels[0].replace("__label__", "")
                        total_score1 = float(scores[0])
                    except Exception as e:
                        total_lang_code = "N/A"
                        total_score1 = 0.0
                        total_text_perplexity = 0.0
                        print(f"\n[Error] Page-level prediction failed for {file_id}, page {page_id}: {e}",
                              file=sys.stderr)

                    # Store page stats for "Short" line context
                    page_stats_cache[page_key] = {
                        "lang": total_lang_code,
                        "score": total_score1,
                        "perplexity": total_text_perplexity
                    }

                    # --- D. Initialize Caches for this page ---
                    line_counts_cache[page_key] = {"Clear": 0, "Noisy": 0, "Trash": 0, "Non-text": 0, "Empty": 0,
                                                   "Rough": 0, "Short": 0}
                    language_counts_cache[page_key] = {lang: 0 for lang in COMMON_LANGS}
                    language_counts_cache[page_key]['other'] = 0

                    # if not page_lines:
                    #     print(f"\t[Warning] No text lines extracted from file '{file_id}', page '{page_id}'")

                    # --- E. Accumulate Lines from this page ---
                    for l_num, line_text in enumerate(page_lines, start=1):

                        # Call the pre-filter to see if line is worth processing
                        category, clean_text = pre_filter_line(line_text)

                        if category == "Empty":
                            lines_writer.writerow([
                                file_id, page_id, l_num, line_text,
                                "N/A", "", 0.0, 0.0, 0.0, 0.0, "Empty", ""
                            ])
                            line_counts_cache[page_key]["Empty"] += 1

                        elif category == "Non-text":
                            lines_writer.writerow([
                                file_id, page_id, l_num, line_text,
                                "N/A", "", 0.0, 0.0, 0.0, 0.0, "Non-text", ""
                            ])
                            line_counts_cache[page_key]["Non-text"] += 1

                        elif category == "Process":
                            # Add this line to our batch accumulators
                            line_batch_to_process.append(clean_text)
                            line_batch_metadata.append({
                                "file_id": file_id,
                                "page_id": page_id,
                                "line_number": l_num,
                                "original_line_text": line_text
                            })

                    # --- F. Process Batches (if full) ---
                    # Use a while loop in case one page adds enough lines for *multiple* batches
                    while len(line_batch_to_process) >= BATCH_SIZE:
                        # 1. Get the batch
                        lines_to_run = line_batch_to_process[:BATCH_SIZE]
                        meta_to_run = line_batch_metadata[:BATCH_SIZE]

                        # 2. Run the batch
                        try:
                            all_line_results = classify_lines_batch(
                                lines_to_run,
                                meta_to_run,
                                page_stats_cache,
                                model_fasttext,
                                model_causal,
                                tokenizer_causal,
                                SPELLERS,
                                device
                            )
                        except Exception as e:
                            print(f"\n[CRITICAL BATCH ERROR] Batch processing failed: {e}", file=sys.stderr)
                            # Create dummy "Trash" results to avoid data loss
                            all_line_results = [{"category": "Trash", "lang_code": "N/A"} for _ in lines_to_run]

                        # 3. Write results and update counts
                        process_batch_results(
                            all_line_results,
                            meta_to_run,
                            lines_writer,
                            line_counts_cache,
                            language_counts_cache
                        )

                        # 4. Clear processed items from accumulators
                        line_batch_to_process = line_batch_to_process[BATCH_SIZE:]
                        line_batch_metadata = line_batch_metadata[BATCH_SIZE:]

                        # 5. Log batch progress
                        batches_processed += 1
                        if batches_processed % LOG_STEP == 0:
                            f_lines_csv.flush()
                            time_spent = time.time() - time_start
                            lines_per_sec = (batches_processed * BATCH_SIZE) / time_spent if time_spent > 0 else 0
                            pages_per_min = (rows_processed / time_spent) * 60 if time_spent > 0 else 0
                            print(
                                f"\n  -> Processed batch {batches_processed:,}. Total time: {(time_spent / 60):.1f} min \t {batches_processed* BATCH_SIZE:,} lines finished.")
                            print(f"  -> Avg speed: {lines_per_sec:.1f} lines/sec \t | \t {pages_per_min:.2f} pages/min")
                            time_left_approx = ((total_rows - rows_processed) / pages_per_min) if pages_per_min > 0 else 0
                            print(f"  -> Estimated time remaining: {time_left_approx:.1f} min\n")

                    # --- G. Log Page Progress ---
                    rows_processed += 1
                    txt_lines_processed += len(page_lines)
                    percentage = (rows_processed / total_rows) * 100
                    time_spent = time.time() - time_start

                    sys.stdout.write(
                        f"\r* * Progress: [ {rows_processed:,} / {total_rows:,} ] pages ({percentage:.1f}%) | Batch Queue: {len(line_batch_to_process):<3} lines | Time: {(time_spent / 60):.1f} min... ")
                    sys.stdout.flush()

            # --- 9. End of All Loops ---
            f_lines_csv.flush()
            print("\n\nAll pages read. Processing final batch...")

            # --- 10. Process any remaining lines ---
            if line_batch_to_process:
                try:
                    all_line_results = classify_lines_batch(
                        line_batch_to_process,
                        line_batch_metadata,
                        page_stats_cache,
                        model_fasttext,
                        model_causal,
                        tokenizer_causal,
                        SPELLERS,
                        device
                    )
                    process_batch_results(
                        all_line_results,
                        line_batch_metadata,
                        lines_writer,
                        line_counts_cache,
                        language_counts_cache
                    )
                    print(f"Processed final batch of {len(line_batch_to_process)} lines.")
                except Exception as e:
                    print(f"\n[CRITICAL FINAL BATCH ERROR] Batch processing failed: {e}", file=sys.stderr)
            else:
                print("No remaining lines to process.")

        # --- 11. Post-Processing and Saving ---
        print("\nProcessing complete.")
        time_total = time.time() - time_start
        time_minutes_total = time_total / 60
        time_hours_total = time_total / 3600
        print(f"Total time: {time_total:.1f} sec ({time_minutes_total:.1f} min, {time_hours_total:.2f} hr)")

        if all_stats_chunks:
            print("Consolidating stats file...")
            # Combine all the *original* chunk DataFrames
            final_stats_df = pd.concat(all_stats_chunks, ignore_index=True)

            # --- Convert caches to DataFrames to merge ---

            # 1. Line Counts
            counts_df = pd.DataFrame.from_dict(line_counts_cache, orient='index')
            # The index is a MultiIndex from (file_id, page_id) tuples.
            # reset_index() will create 'level_0' and 'level_1' columns.
            counts_df.reset_index(inplace=True)

            # *** THIS IS THE FIX ***
            # Rename 'level_0' and 'level_1' directly, and rename all count columns
            counts_df.rename(columns={
                'level_0': 'file',
                'level_1': 'page',
                "Clear": "clear_lines",
                "Noisy": "noisy_lines",
                "Trash": "trash_lines",
                "Non-text": "nontxt_lines",
                "Empty": "empty_lines",
                "Rough": "rough_lines",
                "Short": "short_lines"
            }, inplace=True)
            # *** END OF FIX ***

            # 2. Language Counts
            lang_list = []
            for page_key, lang_counts in language_counts_cache.items():
                sorted_langs = sorted(lang_counts.items(), key=lambda x: x[1], reverse=True)
                top_langs = [lang for lang, count in sorted_langs if count > 0][:2]
                lang_str = "-".join(top_langs) if top_langs else ""
                lang_list.append({'file': page_key[0], 'page': page_key[1], 'languages': lang_str})
            lang_df = pd.DataFrame(lang_list)

            # --- Merge new columns onto the original DataFrame ---
            # Ensure file/page are strings for merging
            final_stats_df['file'] = final_stats_df['file'].astype(str)
            final_stats_df['page'] = final_stats_df['page'].astype(str)
            counts_df['file'] = counts_df['file'].astype(str)
            counts_df['page'] = counts_df['page'].astype(str)
            lang_df['file'] = lang_df['file'].astype(str)
            lang_df['page'] = lang_df['page'].astype(str)

            final_stats_df = final_stats_df.merge(counts_df, on=['file', 'page'], how='left')
            final_stats_df = final_stats_df.merge(lang_df, on=['file', 'page'], how='left')

            # Fill NaNs for pages that had zero processable lines
            count_cols = ['clear_lines', 'noisy_lines', 'trash_lines', 'nontxt_lines', 'empty_lines',
                          'rough_lines', 'short_lines']
            final_stats_df[count_cols] = final_stats_df[count_cols].fillna(0).astype(int)
            final_stats_df['languages'] = final_stats_df['languages'].fillna("")

            print(f"Sorting '{OUTPUT_STATS_FILE}' by file, page...")
            final_stats_df.sort_values(by=["file", "page"], inplace=True)
            final_stats_df.to_csv(OUTPUT_STATS_FILE, index=False, header=True)
            print(f"Updated stats with line counts saved to: {OUTPUT_STATS_FILE}")

            # ... (Rest of your sorting/splitting logic) ...
            print(f"Sorting '{OUTPUT_LINES_FILE}' by file, page, and line. This may take a moment...")
            try:
                df_lines = pd.read_csv(OUTPUT_LINES_FILE)
                df_lines.sort_values(by=["file", "page", "line"], inplace=True)
                df_lines.to_csv(OUTPUT_LINES_FILE, index=False, header=True)
                print("Detailed lines file sorting complete.")
                print(f"Detailed line-by-line analysis saved to: {OUTPUT_LINES_FILE}")

                # --- C. Split results into per-document tables ---
                print(f"Splitting total CSVs into per-document tables in '{OUTPUT_STAT_DIR}'...")
                all_file_ids = final_stats_df['file'].unique()
                split_count = 0
                for file_id in all_file_ids:
                    file_id_str = str(file_id)
                    doc_stats_path = Path(OUTPUT_STAT_DIR) / f"line_counts_{file_id_str}.csv"
                    doc_lines_path = Path(OUTPUT_STAT_DIR) / f"pages_classified_{file_id_str}.csv"

                    # Filter stats DataFrame for this file_id
                    doc_stats_df = final_stats_df[final_stats_df['file'] == file_id]
                    if not doc_stats_df.empty:
                        doc_stats_df.to_csv(doc_stats_path, index=False, header=True)

                    # Filter detailed lines DataFrame for this file_id
                    doc_lines_df = df_lines[df_lines['file'] == file_id]
                    if not doc_lines_df.empty:
                        doc_lines_df.to_csv(doc_lines_path, index=False, header=True)

                    split_count += 1
                print(f"Successfully split results into {split_count} per-document sets.")

            except Exception as e:
                print(f"\n[Error] Failed to sort {OUTPUT_LINES_FILE} or split per-document files: {e}",
                      file=sys.stderr)

        else:
            print("No data processed, stats file and per-document files not created.")


        print(f"Raw text files saved in: {OUTPUT_TEXT_DIR}/")

    except FileNotFoundError:
        print(f"\n[Error] Input file not found at '{INPUT_FILE}'", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()